import streamlit as st
import cv2
import numpy as np
from deepface import DeepFace
from PIL import Image
import pandas as pd
import google.generativeai as genai

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="é¡”ã®è¡¨æƒ…åˆ†æã‚¢ãƒ—ãƒª Integrated", layout="wide")

st.title("ğŸ˜Š è¡¨æƒ…åˆ†æAIã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("æ•°å€¤ã«ã‚ˆã‚‹å®šé‡åˆ†æã¨ã€ç”ŸæˆAIã«ã‚ˆã‚‹å®šæ€§è©•ä¾¡ã‚’çµ±åˆã—ãŸã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚")

# æ—¥æœ¬èªå¤‰æ›ç”¨è¾æ›¸ï¼ˆå®šé‡åˆ†æç”¨ï¼‰
EMOTION_TRANSLATION = {
    "angry": "æ€’ã‚Š",
    "disgust": "å«Œæ‚ª",
    "fear": "æã‚Œ",
    "happy": "å–œã³",
    "sad": "æ‚²ã—ã¿",
    "surprise": "é©šã",
    "neutral": "ç„¡è¡¨æƒ…"
}

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š ---
st.sidebar.header("è¨­å®š")

# ãƒ¢ãƒ¼ãƒ‰é¸æŠ
app_mode = st.sidebar.selectbox(
    "ä½¿ç”¨ã™ã‚‹æ©Ÿèƒ½ã‚’é¸æŠ",
    ("ğŸ“Š æ„Ÿæƒ…ã®å®šé‡åˆ†æ (DeepFace)", "ğŸ“ æ„Ÿæƒ…å¤‰åŒ–ã®å®šæ€§åˆ†æ (Gemini)")
)

# å®šæ€§åˆ†æç”¨APIã‚­ãƒ¼å…¥åŠ›ï¼ˆå®šæ€§åˆ†æãƒ¢ãƒ¼ãƒ‰ã®ã¨ãã ã‘è¡¨ç¤ºã€ã‚ã‚‹ã„ã¯å¸¸æ™‚è¡¨ç¤ºï¼‰
gemini_api_key = st.sidebar.text_input("Google AI Studio API Key", type="password", help="å®šæ€§åˆ†ææ©Ÿèƒ½ã«ã¯APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™")


# ==========================================
# æ©Ÿèƒ½1: æ—¢å­˜ã®å®šé‡åˆ†æ (DeepFace)
# ==========================================
def run_quantitative_analysis():
    st.header("ğŸ“Š æ„Ÿæƒ…ã®å®šé‡åˆ†æ")
    st.write("DeepFaceã‚’ä½¿ç”¨ã—ã¦ã€ç”»åƒã‹ã‚‰æ„Ÿæƒ…æ•°å€¤ã‚’æ¸¬å®šã—ã¾ã™ã€‚")

    min_confidence = st.sidebar.slider("æ¤œå‡ºæ„Ÿåº¦", 0.0, 1.0, 0.5)
    
    input_option = st.radio("å…¥åŠ›ãƒ¢ãƒ¼ãƒ‰", ("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", "ã‚«ãƒ¡ãƒ©ã§æ’®å½±"), horizontal=True)
    input_image = None

    if input_option == "ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
        uploaded_file = st.file_uploader("ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„", type=['jpg', 'png', 'jpeg'])
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            input_image = np.array(image.convert('RGB'))

    elif input_option == "ã‚«ãƒ¡ãƒ©ã§æ’®å½±":
        camera_image = st.camera_input("ã‚«ãƒ¡ãƒ©ã§æ’®å½±ã—ã¦ãã ã•ã„")
        if camera_image is not None:
            image = Image.open(camera_image)
            input_image = np.array(image.convert('RGB'))

    if input_image is not None:
        st.divider()
        col_input, col_btn = st.columns([1, 2])
        with col_input:
            st.image(input_image, caption="å…¥åŠ›ç”»åƒ", use_container_width=True)
        
        with col_btn:
            if st.button("ğŸ” åˆ†æã‚’é–‹å§‹ã™ã‚‹", type="primary"):
                with st.spinner('DeepFaceåˆ†æä¸­...'):
                    try:
                        img_cv = input_image.copy()
                        results = DeepFace.analyze(img_cv, actions=['emotion'], enforce_detection=False)
                        if not isinstance(results, list): results = [results]

                        display_data = []
                        for res in results:
                            region = res['region']
                            emotion_eng = res['dominant_emotion']
                            scores = res['emotion']

                            if region['w'] < 20 or region['h'] < 20: continue

                            x, y, w, h = region['x'], region['y'], region['w'], region['h']
                            cv2.rectangle(img_cv, (x, y), (x+w, y+h), (0, 255, 0), 2)
                            text = f"{emotion_eng} ({scores[emotion_eng]:.2f}%)"
                            cv2.putText(img_cv, text, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (36, 255, 12), 2)

                            formatted_scores = {EMOTION_TRANSLATION.get(k, k): round(v, 2) for k, v in scores.items()}
                            display_data.append({"dominant_jp": EMOTION_TRANSLATION.get(emotion_eng, emotion_eng), "scores": formatted_scores})

                        if not display_data:
                            st.warning("é¡”ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚")
                        else:
                            st.image(img_cv, caption="æ¤œå‡ºçµæœ", use_container_width=True)
                            for i, person in enumerate(display_data):
                                st.subheader(f"ğŸ‘¤ é¡” #{i+1} : {person['dominant_jp']}")
                                df = pd.DataFrame(list(person["scores"].items()), columns=["æ„Ÿæƒ…", "ã‚¹ã‚³ã‚¢ (%)"])
                                df = df.sort_values(by="ã‚¹ã‚³ã‚¢ (%)", ascending=False)
                                st.dataframe(df, hide_index=True, use_container_width=True, column_config={"ã‚¹ã‚³ã‚¢ (%)": st.column_config.ProgressColumn("ç¢ºä¿¡åº¦", format="%.2f%%", min_value=0, max_value=100)})

                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")


# ==========================================
# æ©Ÿèƒ½2: æ–°è¦ã®å®šæ€§åˆ†æ (Gemini Multimodal)
# ==========================================
def run_qualitative_analysis():
    st.header("ğŸ“ æ„Ÿæƒ…å¤‰åŒ–ã®å®šæ€§åˆ†æ")
    st.write("2æšã®ç”»åƒã‚’AIï¼ˆGeminiï¼‰ãŒç›´æ¥è¦–èªã—ã€è¡¨æƒ…ã®å¤‰åŒ–ã‚„é›°å›²æ°—ã‚’è¨€è‘‰ã§å®šæ€§è©•ä¾¡ã—ã¾ã™ã€‚")

    col1, col2 = st.columns(2)
    img1_pil = None
    img2_pil = None

    with col1:
        st.subheader("1. å¤‰åŒ–å‰ (Before)")
        file1 = st.file_uploader("1æšç›®ã‚’é¸æŠ", type=['jpg', 'png', 'jpeg'], key="q_img1")
        if file1:
            img1_pil = Image.open(file1)
            st.image(img1_pil, use_container_width=True)

    with col2:
        st.subheader("2. å¤‰åŒ–å¾Œ (After)")
        file2 = st.file_uploader("2æšç›®ã‚’é¸æŠ", type=['jpg', 'png', 'jpeg'], key="q_img2")
        if file2:
            img2_pil = Image.open(file2)
            st.image(img2_pil, use_container_width=True)

    st.divider()

    if st.button("ğŸ¤– å®šæ€§è©•ä¾¡ã‚’å®Ÿè¡Œã™ã‚‹", type="primary"):
        if not gemini_api_key:
            st.error("âš ï¸ ã‚¨ãƒ©ãƒ¼: ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§Google API Keyã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            return
        
        if img1_pil is None or img2_pil is None:
            st.warning("âš ï¸ ç”»åƒã‚’2æšã¨ã‚‚ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner('GeminiãŒç”»åƒã‚’è¦³å¯Ÿã—ã€ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­...'):
            try:
                # Geminiã®è¨­å®š
                genai.configure(api_key=gemini_api_key)
                
                # ç”»åƒå‡¦ç†ã«ç‰¹åŒ–ã—ãŸè»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
                model = genai.GenerativeModel('gemini-2.0-flash')

                # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ
                prompt = """
                ã‚ãªãŸã¯ç†Ÿç·´ã—ãŸå¿ƒç†ã‚«ã‚¦ãƒ³ã‚»ãƒ©ãƒ¼ã§ã‚ã‚Šã€è¡¨æƒ…åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚
                ä»¥ä¸‹ã®2æšã®ç”»åƒï¼ˆ1æšç›®ãŒå¤‰åŒ–å‰ã€2æšç›®ãŒå¤‰åŒ–å¾Œï¼‰ã‚’è¦‹ã¦ã€äººç‰©ã®è¡¨æƒ…ã‚„é›°å›²æ°—ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã—ãŸã‹ã€å®šæ€§çš„ãªè©•ä¾¡ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

                ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã—ã¦ãã ã•ã„ï¼š
                1. **å…¨ä½“çš„ãªå°è±¡**: ãƒ‘ãƒƒã¨è¦‹ãŸæ™‚ã®é›°å›²æ°—ã®é•ã„ã€‚
                2. **è¡¨æƒ…ã®è©³ç´°ãªå¤‰åŒ–**: ç›®ã¤ãã€å£è§’ã€çœ‰é–“ã®ã‚·ãƒ¯ãªã©ã€é¡”ã®ãƒ‘ãƒ¼ãƒ„ã®å…·ä½“çš„ãªå¤‰åŒ–ã€‚
                3. **æ¨å®šã•ã‚Œã‚‹å¿ƒç†çŠ¶æ…‹**: ã©ã®ã‚ˆã†ãªæ„Ÿæƒ…ã®æ¨ç§»ï¼ˆä¾‹ï¼šç·Šå¼µã‹ã‚‰ç·©å’Œã¸ã€ã‚ã‚‹ã„ã¯æ‚²ã—ã¿ã‹ã‚‰å¸Œæœ›ã¸ãªã©ï¼‰ãŒè¦‹ã¦å–ã‚Œã‚‹ã‹ã€‚
                
                æ–‡ç« ã¯ä¸å¯§ãªæ—¥æœ¬èªã§ã€è¦³å¯Ÿçµæœã«åŸºã¥ã„ãŸæ´å¯Ÿã‚’å«ã‚ã¦ãã ã•ã„ã€‚
                """

                # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«å…¥åŠ› (ãƒ†ã‚­ã‚¹ãƒˆ + ç”»åƒ + ç”»åƒ)
                response = model.generate_content([prompt, img1_pil, img2_pil])
                
                st.subheader("ğŸ“„ åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
                st.markdown(response.text)
                
            except Exception as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")


# ==========================================
# ãƒ¡ã‚¤ãƒ³åˆ†å²
# ==========================================
if app_mode == "ğŸ“Š æ„Ÿæƒ…ã®å®šé‡åˆ†æ (DeepFace)":
    run_quantitative_analysis()
elif app_mode == "ğŸ“ æ„Ÿæƒ…å¤‰åŒ–ã®å®šæ€§åˆ†æ (Gemini)":
    run_qualitative_analysis()